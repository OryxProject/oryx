# Copyright (c) 2014, Cloudera, Inc. All Rights Reserved.
#
# Cloudera, Inc. licenses this file to you under the Apache License,
# Version 2.0 (the "License"). You may not use this file except in
# compliance with the License. You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# This software is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
# CONDITIONS OF ANY KIND, either express or implied. See the License for
# the specific language governing permissions and limitations under the
# License.


# Configuration for the Kafka input queue
input-queue = {

  lock = {
    # Zookeeper master
    master = "localhost:2181"
  }

  message = {
    # Topic for input queue
    topic = "OryxInput"

    # Key/message classes that the framework receives, respectively
    # For now, these have to be the following values as Spark Kafka can only handle Strings
    key-class = "java.lang.String"
    message-class = "java.lang.String"

    # Decoder/encoder classes that the queue uses to read/write key/message classes
    # For now, these have to be the following values as Spark Kafka can only handle Strings
    key-decoder-class = "kafka.serializer.StringDecoder"
    message-decoder-class = "kafka.serializer.StringDecoder"
    key-encoder-class = "kafka.serializer.StringEncoder"
    message-encoder-class = "kafka.serializer.StringEncoder"
  }

}

# Configuration for the Kafka model update queue
update-queue = {

  broker = "localhost:9092"

  lock = {
    # Zookeeper master
    master = "localhost:2181"
  }

  message = {
    # Topic for update queue
    topic = "OryxUpdate"

    # Decoder/encoder classes that the queue uses to read/write key/message classes
    # For now, these have to be the following values as Spark Kafka can only handle Strings
    decoder-class = "kafka.serializer.StringDecoder"
    encoder-class = "kafka.serializer.StringEncoder"
  }

}

# Batch layer configuration
batch = {

  # Streaming framework configuration
  streaming = {
    # Spark Streaming master. If local[n], make sure n >= 2
    master = "local[2]"
  }

  # An implementation of com.cloudera.oryx.lambda.update.BatchLayerUpdate
  # which specifies what is done with current and historical data to update a model
  update-class = null

  # Interval between runs of the computation layer. Default: 6 hours
  generation-interval-sec = 21600

  # Rate at which streaming blocks are created
  # Don't set this unless you know what you're doing
  block-interval-sec = 60

  storage = {

    #Â Directory where historical data is stored. Can be local, or on HDFS, etc.
    data-dir = null
    # Directory where models are output. Can be local, or on HDFS, etc.
    model-dir = null

    # Writable classes used to persist key/message, respectively
    # For now, these have to be the following values as Spark Kafka can only handle Strings
    key-writable-class = "org.apache.hadoop.io.Text"
    message-writable-class = "org.apache.hadoop.io.Text"

    # Max number of data files to write per generation
    # Don't set this unless you know what you're doing
    partitions = 8
  }

}

# Speed layer configuration
speed = {

  # Streaming framework configuration
  streaming = {
    # Spark Streaming master. If local[n], make sure n >= 2
    master = "local[2]"
  }

  model-manager-class = null

  # Interval between runs of the computation layer in msec. Default: 5 seconds
  generation-interval-sec = 5

  # Rate at which streaming blocks are created
  # Don't set this unless you know what you're doing
  block-interval-sec = 1

  # Implementation of com.cloudera.oryx.lambda.speed.SpeedModelManager interface that produces
  # updates from a SpeedModel and stream of input
  model-manager-class = null

}

# Serving layer configuration
serving = {

  api = {
    # Default to use well-known HTTP port for Serving Layer
    port = 8091
    # Default to use well-known HTTPS port for Serving Layer
    secure-port = 443

    # User name for connecting to the service, if required. If set, must be set with password.
    # If enabled, this will enable HTTP DIGEST authentication in the API.
    user-name = null
    # Password for connecting to the service, if required. If set, must be set with user-name.
    # If enabled, this will enable HTTP DIGEST authentication in the API.
    password = null

    # The keystore file containing the server's SSL keys. Only necessary when
    # accessing a server with temporary self-signed key, which is not trusted
    # by the Java SSL implementation.
    keystore-file = null
    # Password needed for keystore file above, if any
    keystore-password = null

    # If true, operations that set or modify data, like /ingest, are not available
    read-only = false
    # An optional prefix for the path under which the service is deployed. For
    # example, set to "/contextPath" to expose services at paths like "http://example.org/contextPath/..."
    context-path = /

  }

  # Implementation of a JAX-RS application exposing end-points
  application-class = null

  # Where to load JAX-RS application resources - package name. 
  # set to value to load als resources until application.conf is integrated
  application-resources = null

  # Implementation of com.cloudera.oryx.lambda.serving.ServingModelManager interface
  # that produces a ServingModel from stream of updates
  model-manager-class = null

}

# ML tier configuration
ml = {

  # Model evaluation settings
  eval = {
    # Fraction of current data that is used for test, versus training
    test-fraction = 0.1
    # Increase to build more candidate models per run, and pick the best one
    candidates = 1
    # Number of models to build in parallel
    parallelism = 1
  }

}
