
<!DOCTYPE html>
<!--
 Generated by Apache Maven Doxia at 2017-09-06
 Rendered using Reflow Maven Skin 1.1.1 (http://andriusvelykis.github.io/reflow-maven-skin)
-->
<html  xml:lang="en" lang="en">

	<head>
		<meta charset="UTF-8" />
		<title>Oryx &#x2013; Docs: Admin</title>
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<meta name="description" content="" />
		<meta http-equiv="content-language" content="en" />

		<link href="http://netdna.bootstrapcdn.com/bootswatch/2.3.2/united/bootstrap.min.css" rel="stylesheet" />
		<link href="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/css/bootstrap-responsive.min.css" rel="stylesheet" />
		<link href="../css/bootswatch.css" rel="stylesheet" />
		<link href="../css/reflow-skin.css" rel="stylesheet" />


		<link href="../css/lightbox.css" rel="stylesheet" />

		<link href="../css/site.css" rel="stylesheet" />
		<link href="../css/print.css" rel="stylesheet" media="print" />

		<!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
		<!--[if lt IE 9]>
			<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->



	</head>

	<body class="page-docs-admin project-oryx" data-spy="scroll" data-offset="60" data-target="#toc-scroll-target">

		<div class="navbar navbar-fixed-top">
			<div class="navbar-inner">
				<div class="container">
					<a class="btn btn-navbar" data-toggle="collapse" data-target="#top-nav-collapse">
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</a>
					<a class="brand" href="../index.html">Oryx 2</a>
					<div class="nav-collapse collapse" id="top-nav-collapse">
						<ul class="nav pull-right">
							<li ><a href="../index.html" title="Overview">Overview</a></li>
							<li ><a href="endusers.html" title="Docs: End Users">Docs: End Users</a></li>
							<li ><a href="developer.html" title="Docs: Dev">Docs: Dev</a></li>
							<li class="active"><a href="" title="Docs: Admin">Docs: Admin</a></li>
							<li ><a href="performance.html" title="Performance">Performance</a></li>
							<li ><a href="../apidocs/index.html" title="Javadoc">Javadoc</a></li>
							<li ><a href="https://github.com/OryxProject/oryx" title="GitHub" class="externalLink">GitHub</a></li>
							<li ><a href="https://github.com/OryxProject/oryx/releases" title="Download" class="externalLink">Download</a></li>
						</ul>
					</div><!--/.nav-collapse -->
				</div>
			</div>
		</div>

	<div class="container">

	<!-- Masthead
	================================================== -->

	<header>
	</header>

	<div class="main-body">
	<div class="row">
		<div class="span8">
			<div class="body-content">
<div class="page-header">
 <h1 id="cluster_setup">Cluster Setup</h1>
</div> 
<p>The following are required as of Oryx 2.5.0:</p> 
<ul> 
 <li>Java 8 or later (JRE only is required)</li> 
 <li>Scala 2.11 or later</li> 
 <li>A Hadoop cluster running the following components: 
  <ul> 
   <li><a class="externalLink" href="https://hadoop.apache.org/">Apache Hadoop</a> 2.7.0 or later</li> 
   <li><a class="externalLink" href="https://zookeeper.apache.org/">Apache Zookeeper</a> 3.4.5 or later</li> 
   <li><a class="externalLink" href="https://kafka.apache.org/">Apache Kafka</a> 0.10.2 or later</li> 
   <li><a class="externalLink" href="https://spark.apache.org/">Apache Spark</a> 2.2.0 or later</li> 
  </ul></li> 
</ul> 
<div class="section"> 
 <h2 id="Requirements_Matrix_Summary">Requirements Matrix Summary</h2> 
 <p>This table summarizes the version of several key components targeted by each version, including older versions that are no longer maintained (EOL).</p> 
 <table border="0" class="bodyTable table table-striped table-hover"> 
  <thead> 
   <tr class="a"> 
    <th>Oryx </th> 
    <th>Java </th> 
    <th>Scala </th> 
    <th>Spark </th> 
    <th>Kafka </th> 
    <th>CDH </th> 
   </tr> 
  </thead> 
  <tbody> 
   <tr class="b"> 
    <td>2.5.x </td> 
    <td>8 </td> 
    <td>2.11 </td> 
    <td>2.2.x </td> 
    <td>0.10.2+ </td> 
    <td>5.12+ </td> 
   </tr> 
   <tr class="a"> 
    <td>2.4.x </td> 
    <td>8 </td> 
    <td>2.11 </td> 
    <td>2.1.x </td> 
    <td>0.10.x </td> 
    <td>5.11 </td> 
   </tr> 
   <tr class="b"> 
    <td>2.3.x (EOL) </td> 
    <td>8 </td> 
    <td>2.11 </td> 
    <td>2.0.x </td> 
    <td>0.9.x </td> 
    <td>5.10 </td> 
   </tr> 
   <tr class="a"> 
    <td>2.2.x (EOL) </td> 
    <td>8 </td> 
    <td>2.10 </td> 
    <td>1.6.x </td> 
    <td>0.9.x </td> 
    <td>5.7 </td> 
   </tr> 
   <tr class="b"> 
    <td>2.1.x (EOL) </td> 
    <td>7 </td> 
    <td>2.10 </td> 
    <td>1.5.x </td> 
    <td>0.8.x </td> 
    <td>5.5 </td> 
   </tr> 
   <tr class="a"> 
    <td>2.0.x (EOL) </td> 
    <td>7 </td> 
    <td>2.10 </td> 
    <td>1.3.x </td> 
    <td>0.8.x </td> 
    <td>5.4 </td> 
   </tr> 
  </tbody> 
 </table> 
</div> 
<div class="section"> 
 <h2 id="Deployment_Architecture">Deployment Architecture</h2> 
 <p>Because the Batch and Speed Layers are Spark applications, they need to run within a cluster. The applications themselves run the driver for these Spark applications, and these may run on an edge node in a cluster like any other Spark application.. That is, the binaries themselves do not need to run on a node that also runs a particular service, but, it will need to run on a node within the cluster because both Layer application interact extensively with compute and storage within the cluster.</p> 
 <p>The Serving Layer may be run within the cluster too, and may be run via YARN on any node. However it’s common to consider deploying this Layer, which exposes an API to external services, on a node that is not within the cluster. This is possible. The Serving Layer must be able to communicate with a Kafka broker, at a minimum.</p> 
 <p>In some applications, the Serving Layer also needs to read large models directly from HDFS. In these cases, it would also have to access HDFS. This is only required in applications that must write large models to HDFS. This is closely related to <tt>oryx.update-topic.message.max-size</tt> and the maximum size message that Kafka can support.</p> 
</div> 
<div class="section"> 
 <h2 id="Services">Services</h2> 
 <p>Install and configure the Hadoop cluster normally. The following services need to be enabled:</p> 
 <ul> 
  <li>HDFS</li> 
  <li>YARN</li> 
  <li>Zookeeper</li> 
  <li>Kafka</li> 
  <li>Spark 2</li> 
 </ul> 
 <p>Note that for CDH 5.x, Spark 2.2 is available as an <a class="externalLink" href="https://www.cloudera.com/documentation/spark2/latest/topics/spark2.html">add on</a>.</p> 
 <p>Kafka is available as a parcel from <a class="externalLink" href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_packaging.html">Cloudera</a>. The Cloudera Kafka 2.2.x parcel is required, because it contains a distribution of Kafka 0.10.2. </p> 
 <p>Determine the (possibly several) Kafka brokers that are configured in the cluster, under Instances, and note their hosts and port. The port is typically 9092. Same for the Zookeeper servers; the default port here is 2181. Default ports will be used in subsequent examples.</p> 
 <p>Where a Kafka broker or Zookeeper server is called for, you can and should specify a comma-separated list of <tt>host:port</tt> pairs where there are multiple hosts. Example: <tt>your-zk-1:2181,your-zk-2:2181</tt>.</p> 
 <p>Also note whether your Zookeeper instance is using a chroot path. This is simply a path suffixed to the host:port, like <tt>your-zk:2181/your-chroot</tt>. It is often <tt>/kafka</tt> if it is set, and this is the default on Cloudera’s distribution, in some versions. You can omit this if you are not using a chroot. </p> 
 <p>Note: if you have multiple Zookeeper servers, and a chroot, only add the chroot once, at the end: <tt>your-zk-1:2181,your-zk-2:2181/kafka</tt></p> 
</div> 
<div class="section"> 
 <h2 id="Java">Java</h2> 
 <p>Java 8 (JRE) needs to be installed on all nodes on the cluster. Cluster processes need to use Java 8. Depending on the nature of your Hadoop cluster installation, this may mean updating the default Java version with <tt>update-alternatives --config java</tt> or equivalent, or setting <tt>JAVA_HOME</tt> to point to a Java 8 installation.</p> 
 <p>For CDH in particular, however, instead see <a class="externalLink" href="https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_java_home_location.html">Configuring a Custom Java Home Location</a></p> 
</div> 
<div class="section"> 
 <h2 id="Configuring_Kafka">Configuring Kafka</h2> 
 <p>Oryx will use two Kafka topics for data transport. One carries input data to the batch and Speed Layer, and the other carries model updates from there on to the Serving Layer. The default names of these topics are <tt>OryxInput</tt> and <tt>OryxUpdate</tt> respectively. They need to be created before Oryx is started.</p> 
 <p>The number of partitions for the <i>input</i> topic will affect the number of partitions, and therefore parallelism, of the Spark Streaming jobs that consume them. For example, the Batch Layer reads partitions of historical data from HDFS and from Kafka. If the input topic has just one partition but a large amount of data arrives per interval, then the Kafka-based partition of the input may be relatively very large and take a long time to process. A good rule of thumb may be to choose a number of topic partitions such that the amount of data that arrives in one batch interval, per partition, is expected to be under the size of one HDFS block, which is 128MB by default. So if you have 1.28GB arriving per batch interval, at least 10 partitions is probably a good idea to make sure the data can be processed in reasonably sized chunks, and with enough parallelism.</p> 
 <p>The provided <tt>oryx-run.sh kafka-setup</tt> script configures a default of 4 partitions, but this can be changed later. Note that there is no purpose in configuring more than 1 partition for the <i>update</i> topic.</p> 
 <p>Replication factor can be any value, but at least 2 is recommended. Note that the replication factor can’t exceed the number of Kafka brokers in the cluster. The provided script sets replication to 1, by default, for this reason. This can be changed later with, for example, <tt>kafka-topics --zookeeper ... --alter --topic ... --replication-factor N</tt></p> 
 <p>You may need to configure the retention time for one or both topics. In particular, it’s typically important to limit the retention time for the update topic, since the Speed and Serving Layer read the entire topic from the start on startup to catch up. This is not as important for the input topic, which is not re-read from the beginning.</p> 
 <p>Setting it to twice the Batch Layer update interval is a good start. For example, to set it to 1 day (24 * 60 * 60 * 1000 = 86400000 ms), set the topic’s <tt>retention.ms</tt> property to 86400000. This is done automatically by the provided <tt>oryx-run.sh kafka-setup</tt> script.</p> 
 <p>The two topics above may contain large messages; in particular the update topic includes entire serialized PMML models. It’s possible that they exceed Kafka’s default max message size of 1 MiB. If large models are expected, then the topic’s <tt>max.message.bytes</tt> should be configured to allow larger messages. <tt>oryx-run.sh kafka-setup</tt> sets a default of 16 MiB for the update topic. This is also the default maximum size of a model that Oryx will attempt to write to the update topic; larger models will be passed as a reference to the model file’s location on HDFS instead. See setting <tt>oryx.update-topic.message.max-size</tt>.</p> 
 <p>The Kafka broker’s <tt>message.max.bytes</tt> (note the different name!) property also controls this, but setting it affects all topics managed by the broker, which may be undesirable. See <a class="externalLink" href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_performance.html">Performance and Resource Considerations</a> for a more complete discussion. In particular, note that <tt>replica.fetch.max.bytes</tt> would have to be set in the broker in order to <i>replicate</i> any very large messages. There is no per-topic equivalent to this.</p> 
 <div class="section"> 
  <h3 id="Automated_Kafka_Configuration">Automated Kafka Configuration</h3> 
  <p>The provided <tt>oryx-run.sh</tt> script can be used to print current configuration for Zookeeper, list existing topics in Kafka, and optionally create the configured input and update topics if needed. </p> 
  <p>You will need to create an Oryx configuration file first, which can be cloned from the example at <a class="externalLink" href="https://github.com/OryxProject/oryx/blob/master/app/conf/als-example.conf">conf/als-example.conf</a> as a start. At least change the Kafka and Zookeeper configuration, as well as topic names, as desired.</p> 
  <p>With this file as <tt>oryx.conf</tt> and any of the layer JAR files in the same directory, run:</p> 
  <div class="source"> 
   <div class="source"> 
    <pre>./oryx-run.sh kafka-setup

Input  ZK:    your-zk:2181
Input  Kafka: your-kafka:9092
Input  topic: OryxInput
Update ZK:    your-zk:2181
Update Kafka: your-kafka:9092
Update topic: OryxUpdate

All available topics:


Input topic OryxInput does not exist. Create it? y
Creating topic OryxInput
Created topic &quot;OryxInput&quot;.
Status of topic OryxInput:
Topic:OryxInput	PartitionCount:4	ReplicationFactor:1	Configs:
	Topic: OryxInput	Partition: 0	Leader: 120	Replicas: 120,121	Isr: 120,121
	Topic: OryxInput	Partition: 1	Leader: 121	Replicas: 121,120	Isr: 121,120
	Topic: OryxInput	Partition: 2	Leader: 120	Replicas: 120,121	Isr: 120,121
	Topic: OryxInput	Partition: 3	Leader: 121	Replicas: 121,120	Isr: 121,120

Update topic OryxUpdate does not exist. Create it? y
Creating topic OryxUpdate
Created topic &quot;OryxUpdate&quot;.
Updated config for topic &quot;OryxUpdate&quot;.
Status of topic OryxUpdate:
Topic:OryxUpdate	PartitionCount:1	ReplicationFactor:1	Configs:retention.ms=86400000,max.message.bytes=16777216
	Topic: OryxUpdate	Partition: 0	Leader: 120	Replicas: 120,121	Isr: 120,121
</pre> 
   </div> 
  </div> 
  <p>To watch messages sent to the input and update topics, to monitor action of the application, try:</p> 
  <div class="source"> 
   <div class="source"> 
    <pre>./oryx-run.sh kafka-tail
Input  ZK:    your-zk:2181
Input  Kafka: your-kafka:9092
Input  topic: OryxInput
Update ZK:    your-zk:2181
Update Kafka: your-kafka:9092
Update topic: OryxUpdate

...output...
</pre> 
   </div> 
  </div> 
  <p>Then in another window, you can feed input, such as the <tt>data.csv</tt> example from the <a href="endusers.html">end user docs</a>, into the input queue to verify it’s working with:</p> 
  <div class="source"> 
   <div class="source"> 
    <pre>./oryx-run.sh kafka-input --input-file data.csv
</pre> 
   </div> 
  </div> 
  <p>If all is well, these processes can be terminated. The cluster is ready to run Oryx.</p> 
 </div> 
</div> 
<div class="section"> 
 <h2 id="HDFS_and_Data_Layout">HDFS and Data Layout</h2> 
 <p>Kafka is the data transport mechanism in Oryx, so data is present in Kafka at least temporarily. However input data is also stored persistently in HDFS for later use. Likewise, models and updates are produced to a Kafka update topic, but models are also persisted to HDFS for later reference.</p> 
 <p>Input data is stored in HDFS under the directory defined by <tt>oryx.batch.storage.data-dir</tt>. Under this directory, subdirectories titled <tt>oryx-[timestamp].data</tt> are created, one for each batch executed by Spark Streaming in the Batch Layer. Here, <tt>timestamp</tt> is the familiar Unix timestamp in milliseconds.</p> 
 <p>Like most “files” output by distributed processes in Hadoop, this is actually a subdirectory containing many <tt>part-*</tt> files. Each file is a <tt>SequenceFile</tt>, where keys and values from the Kafka input topic have been serialized according to a <tt>Writable</tt> class implementation defined by <tt>oryx.batch.storage.key-writable-class</tt> and <tt>oryx.batch.storage.message-writable-class</tt>. By default, this is <tt>TextWritable</tt> and the string representation of keys and messages are recorded.</p> 
 <p>Data may be deleted from this data directory if desired. It will no longer be used in future Batch Layer computations. In particular, note that setting <tt>oryx.batch.storage.max-age-data-hours</tt> to a nonnegative value will configure the Batch Layer to automatically delete data older than the given number of hours.</p> 
 <p>Similarly, machine-learning-oriented applications (which extend <tt>MLUpdate</tt>) output the model chosen by the Batch Layer in each batch interval. It is also persisted in a subdirectory of the directory defined by <tt>oryx.batch.storage.model-dir</tt>. Under this directory are subdirectories named <tt>[timestamp]</tt>, where <tt>timestamp</tt> is again the familiar Unix timestamp in milliseconds.</p> 
 <p>The content of this subdirectory will depend on the application, but typically contains a PMML model called <tt>model.pmml</tt>, and optionally supplementary files that go with the model.</p> 
 <p>This directory exists to record PMML models for archiving and for use by other tools. Its content may be deleted if desired. Note also that setting <tt>oryx.batch.storage.max-age-model-hours</tt> to a nonnegative value will cause models older than the given number of hours to be deleted automatically.</p> 
 <h1 id="handling_failure">Handling Failure</h1> 
 <p>Eventually, you’ll want to stop one or more of the Layers running, or restart it. Or maybe a server decides to die. What happens then? What’s the worst that can happen?</p> 
</div> 
<div class="section"> 
 <h2 id="Data_Loss">Data Loss</h2> 
 <p>Historical data is saved in HDFS, which should be configured for replication. HDFS ensures data is stored reliably. Kafka is also designed to cope with failure when configured to use replication.</p> 
 <p>That is, there is nothing special to do here in order to ensure that data is never completely lost. It is the job of HDFS and Kafka to always be available and not lose data.</p> 
</div> 
<div class="section"> 
 <h2 id="Server_Failure">Server Failure</h2> 
 <p>In general, all three Layer server processes should run continuously, and can and should be restarted immediately if they have to be stopped, or in case of a failure. This can be accomplished with an init script or similar mechanism (not included, yet).</p> 
 <div class="section"> 
  <h3 id="Serving_Layer">Serving Layer</h3> 
  <p>The Serving Layer has no state. On startup, it reads all models and updates available on the update topic. It begins answering queries as soon as any first, valid model is available. For this reason, it’s desirable to limit the retention time for the update topic.</p> 
  <p>The operation of the Serving Layer is not distributed. Each instance is independent, and may stop or start without affecting others.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Speed_Layer">Speed Layer</h3> 
  <p>The Speed Layer also has no state, and also reads all models and updates available on the update topic. It begins producing updates as soon as it has a valid model. It also begins reading from the input topic, and at the moment, always reads from the latest offset.</p> 
  <p>The Speed Layer uses Spark Streaming and Spark for some of its computation. Spark has the responsibility of dealing with failures during computation in the cluster and retrying tasks.</p> 
  <p>Spark Streaming’s Kafka integration can in some cases recover from failure of the receiver that is reading from Kafka. If the entire process dies and is restarted, and <tt>oryx.id</tt> has been set, then reading will be able to resume from the last offset recorded by Kafka. (Otherwise, it will resume reading from the latest offset. This means data that arrived while no Speed Layer was running will not have produced any update.) Also, data that arrives before the Speed Layer has a model is ignored too. It effectively adopts “at most once” semantics.</p> 
  <p>Because the role of the Speed Layer is to provide an approximate, “best effort” update to the last published model, this behavior is generally no problem, and desirable because of its simplicity.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Batch_Layer">Batch Layer</h3> 
  <p>The Batch Layer is the most complex, since it does generate some state:</p> 
  <ul> 
   <li>Historical data, is always persisted to HDFS</li> 
   <li>If the app chooses to, additional state like models can be persisted to HDFS as well as topics</li> 
  </ul> 
  <p>It also is most sensitive to reading data multiple times or not at all, since it is the component that creates the “official” next model.</p> 
  <p>As with the Speed Layer, Spark and Spark Streaming handles many of the failure scenarios during computation. It also manages storing data to HDFS and is responsible for avoiding writing the same data twice.</p> 
  <p>Applications are responsible for recovering their own ‘state’; currently, applications built on the Oryx ML tier write state into unique subdirectories, and will simply produce a new set of state in a new directory when restarted. Previous state, if it exists, will have been completely written or not at all.</p> 
  <p>The Batch Layer also currently adopts the same “at most once” semantics as the Speed Layer. As above, if the entire process dies and is restarted, and <tt>oryx.id</tt> has been set, then reading will be able to resume from the last offset recorded by Kafka, and otherwise, it will resume reading from the latest offset.</p> 
  <h1 id="troubleshooting__faq">Troubleshooting / FAQ</h1> 
 </div> 
</div> 
<div class="section"> 
 <h2 id="Unsupported_majorminor_version_520">Unsupported major.minor version 52.0</h2> 
 <p>This means you are running 7 or earlier somewhere. Oryx requires Java 8 or later. See section above on installing Java 8 and making it available everywhere on the cluster.</p> 
 <p>If you believe that Java 8 is installed, then try setting <tt>JAVA_HOME</tt> explicitly to the location of the Java 8 JRE/JDK home directory before running the Oryx daemons.</p> 
</div> 
<div class="section"> 
 <h2 id="Initial_job_has_not_accepted_any_resources">Initial job has not accepted any resources</h2> 
 <p>The error usually means that your YARN cluster can’t allocate the resources (memory, cores) that your application is requesting. You’ll have to check and increase what YARN can allocate, free up room, or decrease the amount that your app asks for.</p> 
 <p>The relevant YARN settings are:</p> 
 <ul> 
  <li>Container Memory (<tt>yarn.nodemanager.resource.memory-mb</tt>) - the maximum memory that one YARN node has to allocate to containers</li> 
  <li>Container Virtual CPU Cores (<tt>yarn.nodemanager.resource.cpu-vcores</tt>) - same, for cores</li> 
  <li>Container Memory Maximum (<tt>yarn.scheduler.maximum-allocation-mb</tt>) - maximum memory for one container</li> 
  <li>Container Virtual CPU Cores Maximum (<tt>yarn.scheduler.maximum-allocation-vcores</tt>) - maximum cores for one container</li> 
 </ul> 
 <p>The relevant app settings are:</p> 
 <ul> 
  <li><tt>oryx.{batch,speed}.streaming.num-executors</tt> - number of executors (YARN containers) to allocate</li> 
  <li><tt>oryx.{batch,speed}.streaming.executor-cores</tt> - cores to allocate per executor</li> 
  <li><tt>oryx.{batch,speed}.streaming.executor-memory</tt> - memory to allocate per executor</li> 
 </ul> 
</div> 
<div class="section"> 
 <h2 id="Required_executor_memory__MB_is_above_the_max_threshold__MB_of_this_cluster">Required executor memory (… MB) is above the max threshold (… MB) of this cluster</h2> 
 <p>This means your YARN configuration limits the maximum container size that can be requested. Increase the Container Memory Maximum (<tt>yarn.scheduler.maximum-allocation-mb</tt>) to something larger. For Spark, it generally makes sense to allow large containers.</p> 
</div> 
<div class="section"> 
 <h2 id="IllegalArgumentException_Wrong_FS">IllegalArgumentException: Wrong FS</h2> 
 <div class="source"> 
  <div class="source"> 
   <pre>java.lang.IllegalArgumentException: Wrong FS: hdfs:..., expected: file:///
    	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
</pre> 
  </div> 
 </div> 
 <p>This typically means you are using HDFS, but your Hadoop config (e.g. <tt>core-site.xml</tt>, typically in <tt>/etc/hadoop/conf</tt> is not on the classpath. If you’re building a custom <tt>compute-classpath.sh</tt> script make sure to include this directory along with JARs.</p> 
</div> 
<div class="section"> 
 <h2 id="I_need_to_purge_all_previous_data_and_start_again">I need to purge all previous data and start again</h2> 
 <p>Input data exists in the input Kafka topic for a time before being copied into HDFS. So, input potentially exists as unread message in this topic as well as in the HDFS directory defined by <tt>orxy.batch.storage.data-dir</tt>. It’s easy to delete the data in HDFS; it’s harder to ensure older data in the input topic is not read.</p> 
 <p>The simplest solution is to create a new input topic and change configuration to use it. Then, also delete any pre-existing data in HDFS (or use a new directory). Similarly, since the update topic is read from the beginning, it’s easiest to make a new update topic instead.</p> 
 <p>It’s possible to reuse an existing topic name, by removing all its data (difficult, not recommended) or simply deleting and recreating it. If recreating the topic, it’s necessary to reset the consumer offset Oryx will use. This can be done by directly manipulating offsets stored in Zookeeper, to delete them (somewhat hard, not recommended), or by simply switching <tt>oryx.id</tt> to another value.</p> 
</div> 
<div class="section"> 
 <h2 id="Speed_Layer_isnt_producing_updates_but_is_running">Speed Layer isn’t producing updates, but is running</h2> 
 <p>The Speed Layer won’t produce updates until it has loaded a model. Also, check if the Speed Layer’s batches are queued up. If batches are being created faster than they’re processed, then each is waiting longer and longer to start processing, delaying their updates.</p> 
 <h1 id="performance">Performance</h1> 
 <p>See <a href="performance.html">the performance doc</a>.</p> 
</div>
			</div>
		</div>
		<div class="span4">
			<div id="toc-sidebar">
				<div class="well">
					<ul class="nav nav-list">
						<li class="nav-header">Table of Contents</li>
		<li class="dropdown"><a href="#cluster_setup" title="Cluster Setup">Cluster Setup <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Requirements_Matrix_Summary" title="Requirements Matrix Summary">Requirements Matrix Summary</a>
		<li><a href="#Deployment_Architecture" title="Deployment Architecture">Deployment Architecture</a>
		<li><a href="#Services" title="Services">Services</a>
		<li><a href="#Java" title="Java">Java</a>
		<li class="dropdown"><a href="#Configuring_Kafka" title="Configuring Kafka">Configuring Kafka <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Automated_Kafka_Configuration" title="Automated Kafka Configuration">Automated Kafka Configuration</a>
				<li class="divider"></li>
			</ul>
		</li>
		<li><a href="#HDFS_and_Data_Layout" title="HDFS and Data Layout">HDFS and Data Layout</a>
				<li class="divider"></li>
			</ul>
		</li>
		<li class="dropdown"><a href="#handling_failure" title="Handling Failure">Handling Failure <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Data_Loss" title="Data Loss">Data Loss</a>
		<li class="dropdown"><a href="#Server_Failure" title="Server Failure">Server Failure <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Serving_Layer" title="Serving Layer">Serving Layer</a>
		<li><a href="#Speed_Layer" title="Speed Layer">Speed Layer</a>
		<li><a href="#Batch_Layer" title="Batch Layer">Batch Layer</a>
				<li class="divider"></li>
			</ul>
		</li>
			</ul>
		</li>
		<li class="dropdown"><a href="#troubleshooting__faq" title="Troubleshooting / FAQ">Troubleshooting / FAQ <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Unsupported_majorminor_version_520" title="Unsupported major.minor version 52.0">Unsupported major.minor version 52.0</a>
		<li><a href="#Initial_job_has_not_accepted_any_resources" title="Initial job has not accepted any resources">Initial job has not accepted any resources</a>
		<li><a href="#Required_executor_memory__MB_is_above_the_max_threshold__MB_of_this_cluster" title="Required executor memory (… MB) is above the max threshold (… MB) of this cluster">Required executor memory (… MB) is above the max threshold (… MB) of this cluster</a>
		<li><a href="#IllegalArgumentException_Wrong_FS" title="IllegalArgumentException: Wrong FS">IllegalArgumentException: Wrong FS</a>
		<li><a href="#I_need_to_purge_all_previous_data_and_start_again" title="I need to purge all previous data and start again">I need to purge all previous data and start again</a>
		<li><a href="#Speed_Layer_isnt_producing_updates_but_is_running" title="Speed Layer isn’t producing updates, but is running">Speed Layer isn’t producing updates, but is running</a>
				<li class="divider"></li>
			</ul>
		</li>
		<li><a href="#performance" title="Performance">Performance</a>
					</ul>
				</div>
			</div>
		</div>
	</div>
	</div>

	</div><!-- /container -->

	<!-- Footer
	================================================== -->
	<footer class="well">
		<div class="container">
			<div class="row">
				<div class="span9 bottom-nav">
					<ul class="nav nav-list">
					</ul>
				</div>
			</div>
		</div>
	</footer>

	<div class="container subfooter">
		<div class="row">
			<div class="span12">
				<p class="pull-right"><a href="#">Back to top</a></p>
				<p class="copyright">Copyright &copy;2014-2017. All Rights Reserved.</p>
				<p><a href="http://github.com/andriusvelykis/reflow-maven-skin" title="Reflow Maven skin">Reflow Maven skin</a> by <a href="http://andrius.velykis.lt" target="_blank" title="Andrius Velykis">Andrius Velykis</a>.</p>
			</div>
		</div>
	</div>

	<!-- Le javascript
	================================================== -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>

	<script src="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
	<script src="../js/lightbox.min.js"></script>
	<script src="../js/reflow-scroll.js"></script>

	<script src="../js/reflow-skin.js"></script>

	</body>
</html>
