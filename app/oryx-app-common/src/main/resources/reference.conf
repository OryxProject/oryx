# Copyright (c) 2014, Cloudera and Intel, Inc. All Rights Reserved.
#
# Cloudera, Inc. licenses this file to you under the Apache License,
# Version 2.0 (the "License"). You may not use this file except in
# compliance with the License. You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# This software is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
# CONDITIONS OF ANY KIND, either express or implied. See the License for
# the specific language governing permissions and limitations under the
# License.

# Default ALS implementation configuration

oryx = {

  als = {

    # See http://yifanhu.net/PUB/cf.pdf / Collaborative Filtering for Implicit Feedback Datasets
    # for more explanation of these parameters.

    # Number of iterations of ALS to run
    iterations = 10

    # Is input considered 'implicit' (strength-like) data? or rating-like?
    implicit = true
    # Log strength -- use confidence alpha * log(1 + r/eps) instead of alpha * r
    logStrength = false

    # Model hyperparamters
    hyperparams = {
      # Number of latent features
      features = 10
      # Lambda overfitting param
      lambda = 0.001
      # Alpha param
      alpha = 1.0
      # Epsilon, or eps in alpha * log(1 + r/eps); only used if logStrength = true
      epsilon = 0.00001
    }

    # If true, then the items that each user has interacted with are *not* tracked,
    # and may appear in recommendations. This has the advantage of requiring less memory
    # and processing.
    no-known-items = false

    # Fully-qualified class name of an implementation of
    # com.cloudera.oryx.app.api.RescorerProvider, if any is desired. It will be instantiated
    # and used to modify serving results. Several implementations may be specified, separated
    # by commas.
    rescorer-provider-class = null

    decay = {
      # Decay factor applied to old data. This is expressed as a per-day multiplier.
      # For example, with value 0.9, data that is 3 days old will have its strength
      # multiplied by 0.9^3 = 0.72. Must be in (0,1]. Only applicable when implicit = true.
      factor = 1.0

      # Absolute value at which a decayed value is treated as equivalent to 0. For example,
      # with value 0.01, any value that decays to <= 0.01 will be treated as 0, which may
      # mean a user-item interaction is removed.
      zero-threshold = 0.0
    }

    # Consider only approximately this fraction of all items when making recommendations
    # and computing similar items. Candidates are chosen intelligently with locality sensitive
    # hashing. Lower values increase speed but decrease accuracy. Must be > 0. Values below 0.1
    # are likely to noticeably change results.
    sample-rate = 1.0
  }

  # Default K-Means implementation configuration

  kmeans = {

    # Number of iterations of KMeans to run
    iterations = 30

    # Initiaization strategy
    # Possible values are "k-means||" or "random"
    initialization-strategy = "k-means||"

    # Evaluation strategy
    # Possible values for now are DAVIES_BOULDIN, DUNN, SILHOUETTE, SSE
    evaluation-strategy = "SILHOUETTE"

    # Parallel runs for Model selection
    runs = 3

    # Model hyperparamters
    hyperparams = {
      # Number of clusters
      k = 10
    }

  }

  # Default RDF implementation configuration

  rdf = {

    num-trees = 20

    hyperparams = {

      # Only split nodes when the resulting children have at least this many examples:
      # NOT CURRENTLY USED
      min-node-size = 16

      # Only split nodes when a split can be found that gains at least this much information,
      # in nats (not bits)
      # NOT CURRENTLY USED
      min-info-gain-nats = 0.001

      # Maximum number of splits, per feature, to consider at a node.
      max-split-candidates = 100

      # Maximum tree depth
      max-depth = 8

      # Impurity measure. "gini" or "entropy" currently
      impurity = "entropy"

    }

  }

  # Description of input schema, for problems like classification that require some basic
  # schema information, like categorical features and distinct value counts

  input-schema = {

    # Names of features. Either specify a name for all features of the input, or,
    # feature count with num-features. Ex: feature-names = [ "foo", "bar", "baz" ]
    feature-names = []
    # Set this if feature-names has not been set. Features will take names 0, 1, 2 ...
    num-features = 0

    # Features that should be considered an identifier for the datum, if any
    id-features = []
    # Features that should be ignored entirely
    ignored-features = []

    # Specifies which features are numeric and which are categorical. Specify for all
    # non-ignored columns. Set either numeric-features, in which case everything else is
    # considered categorical, or set categorical-features in which case everything else
    # is considered numeric.
    numeric-features = null
    categorical-features = null

    # Which feature is the target to be predicted, if applicable.
    target-feature = null

  }

}
